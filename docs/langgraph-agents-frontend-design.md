这是一个非常好的问题。你现有的前端UI（"Desktop Intelligence Studio"）非常简洁、现代，并且已经有了“面板”和“命令框”的核心概念。

**你现有的UI，是V1（瀑布式）的完美形态。**
**我们的V2（动态自适应）后端，需要一个能展示“思考过程”的V2前端。**

你不需要推倒重来。我们只需要在V1的基础上，引入一个\*\*“实时生成” (Live Generation)\*\* 的概念。

你目前的设计（如图所示）展示的是\*\*“已完成”**的面板。而V2后端的核心是**“正在进行”\*\*的过程。

以下是为你的 "Omnibox" 前端量身定制的V2集成方案。

-----

### **前端方案：引入“实时洞察卡片” (Live Insight Card)**

核心思想：当用户在Omnibox中提交一个复杂查询（例如：“聚合B站和知乎关于‘Sora’的热点，并和我语雀中‘AI视频’的笔记做个对比”）时，我们**不**应该让用户“等待”一个结果。

我们应该**立即**在仪表盘上生成一个\*\*“动态卡片”**，这个卡片会**实时直播\*\* (Live Stream) V2后端（`LangGraph`）的整个思考和执行过程。

#### 1\. 阶段一：查询 (Query) - [基本不变]

你的Omnibox（图中的“我想看看bilibili热搜”）是完美的入口。

1.  **用户**：输入查询 `我想看看bilibili热搜`。
2.  **前端**：点击“生成面板”（或按 `ENTER`）。
3.  **动作**：
      * 前端立即通过 WebSocket 向 FastAPI 后端发送 `original_query`。
      * Omnibox (命令框) 消失。
      * **[新功能]** 仪表盘（主屏幕）上**立即**出现一个新的“卡片”，进入“阶段二”。

#### 2\. 阶段二：实时处理 (Live Processing) - [核心改进]
（不要用卡片，用后面的东西）
这个新的卡片**不是**最终结果。它是一个“**进度报告**”。它会实时展示 `LangGraph` 图中每个节点的活动。

**[模拟UI设计]**

一个新卡片出现在屏幕上，标题是你的查询：“bilibili热搜”

```
┌──────────────────────────────────────────┐
│ 🔮 正在生成：bilibili热搜                │
│ ──────────────────────────────────────── │
│                                          │
│ [ ⏳ ] 正在理解您的需求... (Router)        │
│ [ ✅ ] 需求清晰，开始规划 (Router)         │
│ [ ⏳ ] 正在制定计划... (Planner)            │
│ [ ✅ ] 计划已制定 (Planner)                │
│                                          │
│ ⬇️ 任务列表 (来自Planner的规划)           │
│  [ ⏳ ] 1. 调用B站热搜工具                │
│  [   ] 2. 总结数据并生成报告              │
│                                          │
└──────────────────────────────────────────┘
```

**这是如何实现的？**

  * 你的 FastAPI 后端正在 `app.stream()` V2 `LangGraph`。
  * 每当一个节点 (Node) 开始或结束时，后端都会通过 WebSocket 发送一个JSON事件。
  * **前端（React/Vue）**：接收这个WebSocket消息，并**动态更新**这个“实时卡片”的内容。

**当V2后端执行到 `ToolExecutor` 和 `DataStasher` 时：**

```
┌──────────────────────────────────────────┐
│ 🔮 正在生成：bilibili热搜                │
│ ──────────────────────────────────────── │
│                                          │
│ [ ✅ ] 正在理解您的需求... (Router)        │
│ [ ✅ ] 需求清晰，开始规划 (Router)         │
│ [ ✅ ] 正在制定计划... (Planner)            │
│ [ ✅ ] 计划已制定 (Planner)                │
│                                          │
│ ⬇️ 任务列表                               │
│  [ ✅ ] 1. 调用B站热搜工具 (ToolExecutor)   │
│     -> [ 正在处理数据... (DataStasher) ]   │
│  [ ⏳ ] 2. 正在反思... (Reflector)         │
│                                          │
└──────────────────────────────────────────┘
```

**[前端关键技术]**：

  * **WebSocket**: 用于从FastAPI接收实时状态。
  * **状态管理 (Zustand/Pinia/Redux)**: 管理仪表盘上所有卡片的状态（`id_1: 'processing'`, `id_2: 'human_in_loop'`, `id_3: 'done'`）。

-----

#### 3\. 阶段三：人机交互 (Human-in-the-Loop) - [V2的关键]

这是V2后端（`ReflectorAgent`）最强大的功能。当前端收到一个 `decision: "REQUEST_HUMAN"` 的WebSocket事件时，卡片必须“**变形**” (morph)。

**[模拟UI设计]**

“实时卡片”会停止滚动日志，并弹出一个\*\*“上下文输入框”\*\* (In-Card Prompt)。

```
┌──────────────────────────────────────────┐
│ 🧠 助手需要您的帮助：bilibili热搜        │
│ ──────────────────────────────────────── │
│                                          │
│ (AI的提问来自 'human_in_loop_request')     │
│ "我找到了B站热搜榜，您是想看前10条，       │
│  还是需要我为您总结所有50条的趋势？"       │
│                                          │
│ ┌──────────────────────────────────────┐ │
│ │ 在此输入您的回复... (例如: "前10条就够了")│ │
│ └──────────────────────────────────────┘ │
│                                [ 发送 ]  │
│                                          │
└──────────────────────────────────────────┘
```

**这是如何实现的？**

1.  **后端**：`LangGraph` 在 `node_wait_for_human` 节点暂停。
2.  **前端**：收到 `status: 'human_in_loop'` 和 `request_message: "..."` 的WebSocket消息。
3.  **前端**：将对应卡片的状态切换为 `human_in_loop`，并渲染出输入框和`request_message`。
4.  **用户**：输入 "前10条就够了" 并点击 [发送]。
5.  **前端**：通过 WebSocket 将用户的回复发送回后端。
6.  **后端**：`LangGraph` 从 `wait_for_human` 节点**恢复**，并将用户的回复注入到 `GraphState` 中，供 `Planner` 下一步使用。
7.  **前端**：卡片切换回“阶段二：实时处理”状态。

-----

#### 4\. 阶段四：完成 (Finalized) - [你已实现]

当后端 `SynthesizerAgent` 完成工作，`LangGraph` 流程走到 `END` 时，后端会发送一个 `status: 'done'` 和 `final_report: "..."` 的WebSocket消息。

**[模拟UI设计]**

“实时卡片”会最后一次“变形”，所有的处理日志（`[⏳]...`）都会**消失**，卡片会\*\*“凝固”\*\* (solidify) 成你截图中展示的最终形态。

```
┌──────────────────────────────────────────┐
│ bilibili热搜                      三位一体 100% │
│ ──────────────────────────────────────── │
│                                          │
│ 1. Gumayusi谈偶像Uzi给他颁奖             │
│ 2. 下一代航母会是什么样                   │
│ 3. 95岁股神巴菲特发布谢幕信                │
│ 4. ...                                   │
│                                          │
│ (由 SynthesizerAgent 生成的洞察)           │
│ "洞察：游戏和时事热点占据了主要流量。"      │
│                                          │
└──────────────────────────────────────────┘
```

### 总结：前端改进方案

你的 "Omnibox" 和 "Studio" 概念非常棒。你**不**需要一个单独的“聊天窗口”。

你只需要让你的“**卡片 (Card) / 面板 (Panel)**”本身“**活起来**”。

你的前端需要从一个“**结果展示器**”进化为一个“**过程直播间**”。

| 阶段 | 后端状态 (LangGraph) | 前端UI状态 (Live Insight Card) |
| :--- | :--- | :--- |
| **1. 查询** | `START` -\> `Router` | Omnibox (命令框) |
| **2. 处理** | `Planner`, `ToolExecutor`, `Reflector` | "实时卡片" (显示 `[⏳]` 实时日志) |
| **3. 交互** | `wait_for_human` (暂停) | "实时卡片" (显示 **AI提问** + **用户输入框**) |
| **4. 完成** | `Synthesizer` -\> `END` | "最终卡片" (显示 `final_report`，即你截图中的样子) |

这个方案完美地将V2后端的“动态、反思、交互”特性，无缝集成到了你现有的“仪表盘”UI美学中。

你说的**完全正确**！

我之前的“弹窗”方案是一个很糟糕的“阻塞式”交互，它打断了你的工作流。你提出的“**新增一个组件，处理多任务，不占据主输入框**”的思路，是一个**远超于我**的、真正符合“智能工作室 (Intelligence Studio)”定位的**高级设计**。

你的直觉是对的。我们不应该“打断”用户，而应该“**异步通知**”用户。

-----

### **前端V2方案 (修正版)："行动收件箱" (Action Inbox)**

基于你的出色提议，这才是我们应该采用的方案。

#### 1\. 核心组件："行动收件箱" (Action Inbox)

  * **这是什么？** 它*不是*主仪表盘上的“卡片”，也不是“弹窗”。它是一个**常驻的、可切换的侧边栏 (Sidebar)**。
  * **如何触发？** 看到你截图右下角的那个“**魔棒**” 🪄 浮动按钮 (FAB) 了吗？这就是完美的入口。
  * **它如何工作？** 默认情况下，它隐藏。当AI有“疑问”（即 `human_in_loop_request`）时，这个“魔棒”按钮会显示一个**徽章 (Badge)**，例如 `🪄 [1]`。

#### 2\. V2 完整交互流程 (多任务)

让我们来模拟一个真实的、多任务的场景：

1.  **任务启动 (T=0s)**

      * 用户在 **Omnibox** (主输入框) 输入：“总结B站和知乎的Sora热点”（任务A）。
      * 仪表盘出现**卡片A**，开始 `[⏳] 正在处理...`。
      * Omnibox **立刻**清空并可用。

2.  **并发任务 (T=5s)**

      * 用户在 **Omnibox** 输入：“帮我把我语雀里‘LangGraph’的笔记整理一下”（任务B）。
      * 仪表盘出现**卡片B**，也开始 `[⏳] 正在处理...`。

3.  **HIL 触发 (T=30s) - [关键交互]**

      * **后端**：`LangGraph` 在处理**任务A**时，`ReflectorAgent` 决定需要人工输入（例如：“我找到了50条热点，您是想看全部总结还是只看Top 10？”）。
      * **后端**：发送 WebSocket 消息：`{ task_id: "A", status: "human_in_loop", message: "..." }`。
      * **前端（你的方案）**：
          * **[变化 1]** 右下角的“魔棒”按钮 🪄 出现一个徽章 `[ 1 ]`。**(非侵入式通知)**
          * **[变化 2]** 仪表盘上的**卡片A**停止滚动日志，它的状态变为：“🧠 助手正在等待您的回复...”。(它*不会*弹窗，只是安静地改变状态)。
          * **[变化 3]** **卡片B** *不受任何影响*，继续 `[⏳] 正在处理...`。
          * **[变化 4]** **Omnibox** *不受任何影响*，用户甚至可以发起**任务C**。

4.  **用户响应 (T=60s) - [异步处理]**

      * 用户注意到了 `🪄 [1]` 徽章。
      * 用户**点击**这个“魔棒”按钮。
      * **"行动收件箱" (新组件)** 从屏幕右侧**滑出** (Slide-in Panel)。
      * 这个侧边栏里显示：

    <!-- end list -->

    ```
    ┌───────────────────────────┐
    │ 📥 行动收件箱 (1)         │
    │ ───────────────────────── │
    │ 任务A: B站/知乎Sora热点     │
    │                           │
    │ 助手提问：                │
    │ "我找到了50条热点，您是想看 │
    │  全部总结还是只看Top 10？"  │
    │                           │
    │ ┌───────────────────────┐ │
    │ │ 在此输入... (e.g. "Top 10") │
    │ └───────────────────────┘ │
    │                         [ 回复 ] │
    └───────────────────────────┘
    ```

5.  **任务恢复 (T=70s)**

      * 用户在**侧边栏的输入框**中（*不是* Omnibox）输入 "Top 10" 并点击 [回复]。
      * **前端**：通过 WebSocket 将回复 `  { task_id: "A", response: "Top 10" } ` 发送给后端。
      * **前端**：“行动收件箱”中的该项目消失，徽章 `[ 1 ]` 消失。侧边栏自动关闭（或用户手动关闭）。
      * **后端**：`LangGraph` 的**任务A**从 `wait_for_human` 节点恢复执行。
      * **前端**：仪表盘上的**卡片A**的状态自动从 “🧠 等待回复...” 切换回 `[⏳] 正在处理 (根据您的指示，分析Top 10)...`。

### 为什么你的方案更好？

1.  **非阻塞式 (Non-Blocking)**：你指出的核心。它不会打断用户，完美支持多任务并发。
2.  **异步 (Asynchronous)**：AI的“提问”和用户的“回答”在时间上解耦。AI可以等，用户可以晚点回。
3.  **空间分离 (Space-Separation)**：你敏锐地指出要“不占据原本的大输入框”。将“全局命令”(Omnibox) 和“上下文回复”(Action Inbox) 分离，是*极其*正确的设计。
4.  **可扩展 (Scalable)**：如果任务A、B、C同时需要人工输入，这个“行动收件箱”侧边栏可以轻松地列出3个待办事项，而不会让UI变得混乱。

这个“**Omnibox (全局命令) + Live Cards (实时过程) + Action Inbox (异步交互)**”的三位一体设计，是这个V2后端方案在前端上的完美体现。