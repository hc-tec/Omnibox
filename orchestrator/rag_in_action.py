"""
RAG-in-Action å®Œæ•´æµç¨‹
èŒè´£ï¼šåè°ƒRAGæ£€ç´¢å’ŒæŸ¥è¯¢è§£æï¼Œå®ç°ç«¯åˆ°ç«¯çš„å¤„ç†
"""
import logging
import sys
from pathlib import Path
from typing import Dict, Any, Optional, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from rag_system.rag_pipeline import RAGPipeline
from query_processor.llm_client import LLMClient, create_llm_client
from query_processor.prompt_builder import PromptBuilder
from query_processor.parser import QueryParser
from query_processor.path_builder import PathBuilder

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RAGInAction:
    """
    RAG-in-Action ä¸»æµç¨‹
    æ•´åˆå‘é‡æ£€ç´¢ + LLMè§£æ = è‡ªç„¶è¯­è¨€ â†’ APIè°ƒç”¨
    """

    def __init__(
        self,
        rag_pipeline: RAGPipeline,
        llm_client: LLMClient,
        prompt_builder: Optional[PromptBuilder] = None,
        path_builder: Optional[PathBuilder] = None,
        retrieval_top_k: int = 3,
        llm_temperature: float = 0.1,
    ):
        """
        åˆå§‹åŒ–RAG-in-Actionæµç¨‹

        Args:
            rag_pipeline: RAGæ£€ç´¢ç®¡é“
            llm_client: LLMå®¢æˆ·ç«¯
            prompt_builder: Promptæ„å»ºå™¨ï¼ˆå¯é€‰ï¼‰
            path_builder: è·¯å¾„æ„å»ºå™¨ï¼ˆå¯é€‰ï¼‰
            retrieval_top_k: RAGæ£€ç´¢è¿”å›çš„ç»“æœæ•°é‡
            llm_temperature: LLMç”Ÿæˆæ¸©åº¦
        """
        self.rag_pipeline = rag_pipeline
        self.llm_client = llm_client
        self.prompt_builder = prompt_builder or PromptBuilder()
        self.path_builder = path_builder or PathBuilder()
        self.query_parser = QueryParser(llm_client)
        self.retrieval_top_k = retrieval_top_k
        self.llm_temperature = llm_temperature

        logger.info("âœ“ RAG-in-Action æµç¨‹åˆå§‹åŒ–å®Œæˆ")

    def process(
        self,
        user_query: str,
        filter_datasource: Optional[str] = None,
        verbose: bool = True,
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢

        å®Œæ•´æµç¨‹ï¼š
        1. RAGæ£€ç´¢ï¼šæ‰¾åˆ°ç›¸å…³çš„è·¯ç”±å®šä¹‰
        2. æ„å»ºPromptï¼šå°†è·¯ç”±å®šä¹‰å’Œç”¨æˆ·æŸ¥è¯¢ç»„åˆ
        3. LLMè§£æï¼šæå–æ„å›¾å’Œå‚æ•°
        4. è·¯å¾„æ„å»ºï¼šç”Ÿæˆå®Œæ•´çš„APIè°ƒç”¨è·¯å¾„

        Args:
            user_query: ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            filter_datasource: è¿‡æ»¤ç‰¹å®šæ•°æ®æºï¼ˆå¯é€‰ï¼‰
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—

        Returns:
            å¤„ç†ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - status: çŠ¶æ€ï¼ˆsuccess/needs_clarification/not_found/errorï¼‰
            - generated_path: ç”Ÿæˆçš„APIè·¯å¾„
            - selected_tool: é€‰ä¸­çš„å·¥å…·ä¿¡æ¯
            - parameters_filled: å¡«å……çš„å‚æ•°
            - reasoning: LLMçš„æ¨ç†è¿‡ç¨‹
            - clarification_question: å¦‚æœéœ€è¦æ¾„æ¸…çš„é—®é¢˜
            - retrieved_tools: RAGæ£€ç´¢åˆ°çš„æ‰€æœ‰å·¥å…·
        """
        if verbose:
            logger.info("="*80)
            logger.info(f"å¼€å§‹å¤„ç†æŸ¥è¯¢: {user_query}")
            logger.info("="*80)

        try:
            # ========== é˜¶æ®µ1: RAGæ£€ç´¢ ==========
            if verbose:
                logger.info("\n[é˜¶æ®µ1] RAGå‘é‡æ£€ç´¢")
                logger.info("-" * 80)

            rag_results = self.rag_pipeline.search(
                query=user_query,
                top_k=self.retrieval_top_k,
                filter_datasource=filter_datasource,
                verbose=verbose,
            )

            if not rag_results:
                return {
                    "status": "not_found",
                    "reasoning": "æœªæ‰¾åˆ°ç›¸å…³çš„APIå·¥å…·",
                    "generated_path": None,
                    "selected_tool": None,
                    "parameters_filled": {},
                    "clarification_question": "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„åŠŸèƒ½ã€‚è¯·æ¢ä¸€ç§æ–¹å¼æè¿°æ‚¨çš„éœ€æ±‚ã€‚",
                    "retrieved_tools": [],
                }

            # æå–è·¯ç”±å®šä¹‰
            retrieved_tools = [route_def for _, _, route_def in rag_results]

            if verbose:
                logger.info(f"âœ“ æ£€ç´¢åˆ° {len(retrieved_tools)} ä¸ªç›¸å…³å·¥å…·")

            # ========== é˜¶æ®µ2: æ„å»ºPrompt ==========
            if verbose:
                logger.info("\n[é˜¶æ®µ2] æ„å»ºLLM Prompt")
                logger.info("-" * 80)

            prompt = self.prompt_builder.build(
                user_query=user_query,
                tools=retrieved_tools,
            )

            if verbose:
                logger.info(f"âœ“ Promptæ„å»ºå®Œæˆï¼ˆé•¿åº¦: {len(prompt)} å­—ç¬¦ï¼‰")

            # ========== é˜¶æ®µ3: LLMè§£æ ==========
            if verbose:
                logger.info("\n[é˜¶æ®µ3] LLMæŸ¥è¯¢è§£æ")
                logger.info("-" * 80)

            parse_result = self.query_parser.parse(
                prompt=prompt,
                temperature=self.llm_temperature,
            )

            if verbose:
                logger.info(f"âœ“ è§£æå®Œæˆ: status={parse_result.get('status')}")
                if parse_result.get('reasoning'):
                    logger.info(f"  æ¨ç†: {parse_result['reasoning']}")

            # ========== é˜¶æ®µ4: è·¯å¾„æ„å»ºï¼ˆå¦‚æœæˆåŠŸï¼‰ ==========
            if parse_result["status"] == "success":
                if verbose:
                    logger.info("\n[é˜¶æ®µ4] æ„å»ºAPIè·¯å¾„")
                    logger.info("-" * 80)

                # ä»retrieved_toolsä¸­æ‰¾åˆ°å¯¹åº”çš„å®Œæ•´è·¯ç”±å®šä¹‰
                selected_route_id = parse_result["selected_tool"]["route_id"]
                selected_route_def = None

                for route_def in retrieved_tools:
                    if route_def.get("route_id") == selected_route_id:
                        selected_route_def = route_def
                        break

                if selected_route_def:
                    # ä½¿ç”¨PathBuilderé‡æ–°æ„å»ºè·¯å¾„ï¼ˆéªŒè¯ï¼‰
                    verified_path = self.path_builder.build(
                        route_def=selected_route_def,
                        parameters=parse_result["parameters_filled"],
                    )

                    parse_result["generated_path"] = verified_path

                    if verbose:
                        logger.info(f"âœ“ è·¯å¾„å·²æ„å»º: {verified_path}")

            # ========== è¿”å›ç»“æœ ==========
            parse_result["retrieved_tools"] = retrieved_tools

            if verbose:
                logger.info("\n" + "="*80)
                logger.info("å¤„ç†å®Œæˆï¼")
                logger.info("="*80)
                self._print_result(parse_result)

            return parse_result

        except Exception as e:
            logger.error(f"å¤„ç†å¤±è´¥: {e}", exc_info=True)

            return {
                "status": "error",
                "reasoning": f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "generated_path": None,
                "selected_tool": None,
                "parameters_filled": {},
                "clarification_question": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é”™è¯¯ã€‚",
                "retrieved_tools": [],
            }

    def _print_result(self, result: Dict[str, Any]) -> None:
        """æ‰“å°å¤„ç†ç»“æœï¼ˆç¾åŒ–è¾“å‡ºï¼‰"""
        print("\nğŸ“Š å¤„ç†ç»“æœ:")
        print(f"  çŠ¶æ€: {result['status']}")

        if result['status'] == 'success':
            print(f"\nâœ… æˆåŠŸç”ŸæˆAPIè°ƒç”¨:")
            print(f"  è·¯å¾„: {result['generated_path']}")
            print(f"  å·¥å…·: {result['selected_tool']['name']} ({result['selected_tool']['route_id']})")
            print(f"  å‚æ•°: {result['parameters_filled']}")

        elif result['status'] == 'needs_clarification':
            print(f"\nâ“ éœ€è¦æ›´å¤šä¿¡æ¯:")
            print(f"  é—®é¢˜: {result.get('clarification_question')}")

        elif result['status'] == 'not_found':
            print(f"\nâŒ æœªæ‰¾åˆ°åŒ¹é…çš„åŠŸèƒ½")

        elif result['status'] == 'error':
            print(f"\nâŒ å¤„ç†é”™è¯¯:")
            print(f"  åŸå› : {result.get('reasoning')}")


# ä¾¿æ·å‡½æ•°
def create_rag_in_action(
    llm_provider: str = "openai",
    llm_config: Optional[Dict] = None,
    rag_pipeline: Optional[RAGPipeline] = None,
) -> RAGInAction:
    """
    ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºRAG-in-Actionå®ä¾‹

    Args:
        llm_provider: LLMæä¾›å•†ï¼ˆopenai/anthropic/customï¼‰
        llm_config: LLMé…ç½®å­—å…¸
        rag_pipeline: RAGç®¡é“ï¼ˆNoneåˆ™è‡ªåŠ¨åˆ›å»ºï¼‰

    Returns:
        RAGInActionå®ä¾‹

    Example:
        >>> ria = create_rag_in_action(
        ...     llm_provider="openai",
        ...     llm_config={"model": "gpt-4"}
        ... )
        >>> result = ria.process("è™æ‰‘æ­¥è¡Œè¡—æœ€æ–°å¸–å­")
    """
    # åˆ›å»ºRAGç®¡é“
    if rag_pipeline is None:
        rag_pipeline = RAGPipeline()

    # åˆ›å»ºLLMå®¢æˆ·ç«¯
    llm_config = llm_config or {}
    llm_client = create_llm_client(llm_provider, **llm_config)

    # åˆ›å»ºRAG-in-Action
    return RAGInAction(
        rag_pipeline=rag_pipeline,
        llm_client=llm_client,
    )
